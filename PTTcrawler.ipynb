{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import traceback\n",
    "import string\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#主要會改的地方\n",
    "#在cell6\n",
    "#url_list = [版名]\n",
    "#在cell7\n",
    "#path = r\"儲存路徑\\PTT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_url(path,flag,article_url):\n",
    "    try:\n",
    "        cur_flag = flag\n",
    "        r = requests.get(article_url)\n",
    "        soup = BeautifulSoup(r.text, \"lxml\")\n",
    "        results = soup.select('span.article-meta-value')\n",
    "        tags = soup.select('span.article-meta-tag')\n",
    "        presults = soup.select('div.bbs-content')\n",
    "        tag_list = []\n",
    "        for i in range(0,len(tags)-1):\n",
    "            tag_list.append(tags[i].text)\n",
    "        print(len(results))\n",
    "        if(len(results) == 4):\n",
    "            title = maketitle(results)\n",
    "            cur_flag = int(title[0:title.find(\"_\")])\n",
    "            content = makecontent(presults[2].text)\n",
    "            if(len(content)>=100):\n",
    "                print(\"title\",title)\n",
    "                titleurl = title + \"\\n\" + article_url + \"\\n\" \n",
    "                save_to_file(path,\"url.txt\",titleurl)\n",
    "            else:\n",
    "                print(len(content),\"<100\",\"title\",title)\n",
    "        else:\n",
    "            try:\n",
    "                up = \"作者\"\n",
    "                p = \"標題_\"+article_url[article_url.find(\"shoes/\")+6:article_url.find(\".html\")]\n",
    "                date = \"0\"\n",
    "                for i in range(0,len(results)-1):\n",
    "                    print(i)\n",
    "                    if(tags[i].text == \"時間\"):\n",
    "                        time = results[i].text\n",
    "                        y=time[-4:len(time)]\n",
    "                        m=findmonth(time[4:7])\n",
    "                        d=finddate(time[8:10])\n",
    "                        date=y+m+d                \n",
    "                        print(\"time:\",date)\n",
    "                    elif(tags[i].text == \"作者\"):\n",
    "                        up = results[i].text[0:results[i].text.index(\"(\")]\n",
    "                        print(\"作者:\",up)\n",
    "                    elif(tags[i].text == \"標題\"):\n",
    "                        p = results[i].text.replace(\"\\\\\",\"_\").replace(\"|\",\"_\").replace(\":\",\"_\").replace(\"<\",\"_\").replace(\">\",\"_\").replace(r'*',\"_\").replace(r'\"',\"_\").replace(r'?',\"_\").replace(r'\\\\',\"/\").replace(r'/',\"_\")+up+\".txt\"\n",
    "                        print(\"標題:\",p)\n",
    "\n",
    "                content = makecontent(presults[2].text)\n",
    "                try:\n",
    "                    if(content.find(\"時間:\")>=1):\n",
    "                        time=content[content.index(\"時間\")+4:content.index(\"時間\")+28]\n",
    "                        try:\n",
    "                            int(time)\n",
    "                            y=time[-4:len(time)]\n",
    "                            m=findmonth(time[4:7])\n",
    "                            d=finddate(time[8:10])\n",
    "                            date=y+m+d\n",
    "                            print(\"time:\",date)    \n",
    "                        except:\n",
    "                            date = \"0\"\n",
    "                        content = content.replace(content[content.index(\"時間\"):content.index(\"時間\")+28],\"\")\n",
    "                    if(content.find(\"標題:\")>=1):\n",
    "                        p=content[content.index(\"標題\")+4:content[content.index(\"標題\"):len(content)-1].index(\"\\n\")+1]\n",
    "                        print(\"標題:\",p)\n",
    "                        content = content.replace(content[content.index(\"標題\"):content[content.index(\"標題\"):len(content)-1].index(\"\\n\")+1],\"\\n\")\n",
    "                    if(content.find(\"作者:\")>=1):\n",
    "                        up=content[content.index(\"作者\")+4:content.index(\"(\")]\n",
    "                        print(\"作者:\",up)\n",
    "                        content = content.replace(content[content.index(\"作者\"):content.index(\")\")+1],\"\\n\")\n",
    "                    title = date + \"_\" + p + \"_\" + up + \".txt\"\n",
    "                    cur_flag = int(title[0:title.find(\"_\")])\n",
    "                    if(len(content)>=100):\n",
    "                        print(\"title\",title)\n",
    "                        titleurl = title + \"\\n\" + article_url + \"\\n\" \n",
    "                        save_to_file(path,\"url\",titleurl)\n",
    "                    else:\n",
    "                        print(len(content),\"<100\",\"title\",title)\n",
    "                except:\n",
    "                    print('不明的程式中斷')\n",
    "                    traceback.print_exc()\n",
    "            except:\n",
    "                print('不明的程式中斷')\n",
    "                traceback.print_exc()\n",
    "    except:\n",
    "        cur_flag = 0\n",
    "        print('不明的程式中斷')\n",
    "        traceback.print_exc()\n",
    "    return cur_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_file(path,file_name, contents):\n",
    "    fh = open(path+\"/\"+file_name,'a',encoding=\"utf-8_sig\")\n",
    "    fh.write(contents)\n",
    "    fh.close()\n",
    "\n",
    "def findmonth(month):\n",
    "    if month==\"Jan\":\n",
    "        return \"01\"\n",
    "    if month==\"Feb\":\n",
    "        return \"02\"\n",
    "    if month==\"Mar\":\n",
    "        return \"03\"\n",
    "    if month==\"Apr\":\n",
    "        return \"04\"\n",
    "    if month==\"May\":\n",
    "        return \"05\"\n",
    "    if month==\"Jun\":\n",
    "        return \"06\"\n",
    "    if month==\"Jul\":\n",
    "        return \"07\"\n",
    "    if month==\"Aug\":\n",
    "        return \"08\"\n",
    "    if month==\"Sep\":\n",
    "        return \"09\"\n",
    "    if month==\"Oct\":\n",
    "        return \"10\"\n",
    "    if month==\"Nov\":\n",
    "        return \"11\"\n",
    "    if month==\"Dec\":\n",
    "        return \"12\"\n",
    "    \n",
    "def finddate(date):\n",
    "    if date==\" 1\":\n",
    "        return \"01\"\n",
    "    if date==\" 2\":\n",
    "        return \"02\"\n",
    "    if date==\" 3\":\n",
    "        return \"03\"\n",
    "    if date==\" 4\":\n",
    "        return \"04\"\n",
    "    if date==\" 5\":\n",
    "        return \"05\"\n",
    "    if date==\" 6\":\n",
    "        return \"06\"\n",
    "    if date==\" 7\":\n",
    "        return \"07\"\n",
    "    if date==\" 8\":\n",
    "        return \"08\"\n",
    "    if date==\" 9\":\n",
    "        return \"09\"\n",
    "    else:\n",
    "        return date\n",
    "\n",
    "def sweeper(al):\n",
    "    al.remove(al[0])\n",
    "    revind=[]\n",
    "    ind=0\n",
    "    for aa in al:\n",
    "        if(aa.count(\"推\")>=1 or aa.count(\"→\")>=1 or aa.count(\"噓\")>=1):\n",
    "            if(aa.count(\":\")>=1):\n",
    "                if(aa.count(\"http\")>=1):\n",
    "                    revind.append(aa)\n",
    "                else:\n",
    "                    alist=[]\n",
    "                    flag =0\n",
    "                    for i in range(flag,len(aa)-1):\n",
    "                        if (aa[i] == \" \"):\n",
    "                            nc = aa[flag:i]\n",
    "                            flag=i+1\n",
    "                            alist.append(nc)\n",
    "                    item=\"\"\n",
    "                    for itemindex in range(2,len(alist)-1):\n",
    "                        item=item+alist[itemindex]+\" \"\n",
    "                    al[ind]=item\n",
    "        if(aa.count(\"※\")>=1 or aa.count(\"--\")>=1 or aa.count(\"http\")>=1 or aa.count(\"◆\")>=1):\n",
    "            if(aa not in revind):\n",
    "                revind.append(aa)\n",
    "        ind=ind+1\n",
    "    for i in revind:\n",
    "        if(i in al):\n",
    "            al.remove(i)\n",
    "    content = \"\"\n",
    "    for aa in al:\n",
    "        content = content+aa+\"\\n\"\n",
    "    return content\n",
    "\n",
    "def findup(target):\n",
    "    i=target.find(\" \")\n",
    "    up='_'+target[0:i]\n",
    "    return up\n",
    "\n",
    "def maketitle(target):\n",
    "    y=target[3].text[-4:len(target[3].text)]\n",
    "    m=findmonth(target[3].text[4:7])\n",
    "    d=finddate(target[3].text[8:10]) \n",
    "    try:\n",
    "        date=y+m+d\n",
    "        int(date)\n",
    "        str(date)\n",
    "    except:\n",
    "        \n",
    "        print(\"ymd\",y,m,d)\n",
    "        date =\"0\"\n",
    "    up=findup(target[0].text)\n",
    "    a=date+\"_\"+target[2].text.replace(\"\\\\\",\"_\").replace(\"|\",\"_\").replace(\":\",\"_\").replace(\"<\",\"_\").replace(\">\",\"_\").replace(r'*',\"_\").replace(r'\"',\"_\").replace(r'?',\"_\").replace(r'\\\\',\"/\").replace(r'/',\"_\")+up+\".txt\"\n",
    "    return a\n",
    "\n",
    "def makecontent(target):\n",
    "    al = []\n",
    "    flag = 0\n",
    "    for i in range(flag, len(target)-1):\n",
    "        if (target[i] == \"\\n\"):\n",
    "            row = target[flag:i]\n",
    "            flag=i+1\n",
    "            al.append(row)\n",
    "    al.append(target[flag:len(target)-1])\n",
    "    content = sweeper(al)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_all_href(flag,url):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "    results = soup.select(\"div.title\")\n",
    "    cur_flag = flag\n",
    "    stop_flag = []\n",
    "    for item in results:\n",
    "        a_item = item.select_one(\"a\")\n",
    "        if a_item:\n",
    "            a_flag = get_article_url(path,flag,article_url='https://www.ptt.cc'+a_item.get('href'))\n",
    "            if(a_flag>=cur_flag):\n",
    "                cur_flag = a_flag\n",
    "            if(a_flag >= flag):\n",
    "                print(a_flag, \">=\", flag,\":\",a_flag >= flag)\n",
    "                stop_flag.append(0)\n",
    "                print(\"0\")\n",
    "            else:\n",
    "                stop_flag.append(1)\n",
    "                print(\"1\")\n",
    "            \n",
    "            print(\"cur_flag\",cur_flag)\n",
    "    print(stop_flag)\n",
    "    print('------------------ next page ------------------')\n",
    "    return stop_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = [\n",
    "   \"shoes\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in url_list:\n",
    "    url=\"https://www.ptt.cc/bbs/\"+url+\"/index.html\"\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "    print('url:', url)\n",
    "    boardname = url[23:url.find(r\"/index\")]\n",
    "    print(boardname)\n",
    "    path = r\"C:\\Users\\Suisei_Saika\\Desktop\\PTT\"+boardname\n",
    "    print(path)\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "    flag = 0\n",
    "    f = os.listdir(path)\n",
    "    stop_flag = get_all_href(flag,url) \n",
    "    if(0 in stop_flag and 1 in stop_flag):\n",
    "        if(stop_flag.index(1)>stop_flag.index(0)):\n",
    "            btn = soup.select('div.btn-group > a')\n",
    "            if btn:\n",
    "                next_page_url = 'https://www.ptt.cc' + btn[3]['href']\n",
    "                url = next_page_url\n",
    "                print('url:',url)\n",
    "                ii=url.index('index')\n",
    "                hi=url.index('.html')\n",
    "                pages=url[ii+5:hi]\n",
    "                stop_flag = get_all_href(flag,url = url) \n",
    "                for page in range(1,int(pages)):\n",
    "                    if(0 in stop_flag and 1 in stop_flag):\n",
    "                        if(stop_flag.index(1)>stop_flag.index(0)):\n",
    "                            r = requests.get(url)\n",
    "                            soup = BeautifulSoup(r.text, \"lxml\")\n",
    "                            btn = soup.select('div.btn-group > a')\n",
    "                            if btn:\n",
    "                                next_page_url = 'https://www.ptt.cc' + btn[3]['href']\n",
    "                                url = next_page_url\n",
    "                                print('url:',url)\n",
    "                                stop_flag = get_all_href(flag,url = url)\n",
    "                        else:\n",
    "                            print(\"資料為最新\")\n",
    "                            break\n",
    "                    elif(1 not in stop_flag):\n",
    "                        r = requests.get(url)\n",
    "                        soup = BeautifulSoup(r.text, \"lxml\")\n",
    "                        btn = soup.select('div.btn-group > a')\n",
    "                        if btn:\n",
    "                            next_page_url = 'https://www.ptt.cc' + btn[3]['href']\n",
    "                            url = next_page_url\n",
    "                            print('url:',url)\n",
    "                            stop_flag = get_all_href(flag,url = url)\n",
    "        else:\n",
    "            print(\"資料為最新\")\n",
    "    elif(1 not in stop_flag):\n",
    "        btn = soup.select('div.btn-group > a')\n",
    "        if btn:\n",
    "            next_page_url = 'https://www.ptt.cc' + btn[3]['href']\n",
    "            url = next_page_url\n",
    "            print('url:',url)\n",
    "            ii=url.index('index')\n",
    "            hi=url.index('.html')\n",
    "            pages=url[ii+5:hi]\n",
    "            stop_flag = get_all_href(flag,url = url) \n",
    "            for page in range(1,int(pages)):\n",
    "                if(0 in stop_flag and 1 in stop_flag):\n",
    "                    if(stop_flag.index(1)>stop_flag.index(0)):\n",
    "                        r = requests.get(url)\n",
    "                        soup = BeautifulSoup(r.text, \"lxml\")\n",
    "                        btn = soup.select('div.btn-group > a')\n",
    "                        if btn:\n",
    "                            next_page_url = 'https://www.ptt.cc' + btn[3]['href']\n",
    "                            url = next_page_url\n",
    "                            print('url:',url)\n",
    "                            stop_flag = get_all_href(flag,url = url)\n",
    "                    else:\n",
    "                        print(\"資料為最新\")\n",
    "                        break\n",
    "                elif(1 not in stop_flag):\n",
    "                    r = requests.get(url)\n",
    "                    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "                    btn = soup.select('div.btn-group > a')\n",
    "                    if btn:\n",
    "                        next_page_url = 'https://www.ptt.cc' + btn[3]['href']\n",
    "                        url = next_page_url\n",
    "                        print('url:',url)\n",
    "                        stop_flag = get_all_href(flag,url = url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# url='https://www.ptt.cc/bbs/shoes/index921.html'\n",
    "# r = requests.get(url)\n",
    "# soup = BeautifulSoup(r.text, \"lxml\")\n",
    "# print('url:', url)\n",
    "# get_all_href(url) \n",
    "# btn = soup.select('div.btn-group > a')\n",
    "# if btn:\n",
    "#     next_page_url = 'https://www.ptt.cc' + btn[3]['href']\n",
    "#     url = next_page_url\n",
    "#     print('url:',url)\n",
    "#     ii=url.index('index')\n",
    "#     hi=url.index('.html')\n",
    "#     pages=url[ii+5:hi]\n",
    "#     get_all_href(url = url) \n",
    "# for page in range(1,int(pages)):\n",
    "#     r = requests.get(url)\n",
    "#     soup = BeautifulSoup(r.text, \"lxml\")\n",
    "#     btn = soup.select('div.btn-group > a')\n",
    "#     if btn:\n",
    "#         next_page_url = 'https://www.ptt.cc' + btn[3]['href']\n",
    "#         url = next_page_url\n",
    "#         print('url:',url)\n",
    "#         get_all_href(url = url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
